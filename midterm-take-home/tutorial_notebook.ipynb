{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for running the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial on running the CNN to generate the information plane graph mentioned. My CNN consists of a simple network containing 2 convolutional layers, 2 fully connected layers, and one dropout layer. I ran the CNN for 7 epochs due to hardware limitations. My system kept crashing whenever I tried to run it for more than 7 epochs owing to low memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN was added to IDNNs/idnns/networks/models.py where the exisitng models were already present. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program starts by running IDNNs/main.py\n",
    "\n",
    "```\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "from idnns.networks import information_network as inet\n",
    "def main():\n",
    "    #Build the network\n",
    "    print ('Building the network')\n",
    "    net = inet.informationNetwork()\n",
    "    net.print_information()\n",
    "\n",
    "    print ('Start running the network')\n",
    "    net.run_network()\n",
    "\n",
    "    print ('Saving data')\n",
    "    net.save_data()\n",
    "\n",
    "    print ('Ploting figures')\n",
    "    #Plot the newtork\n",
    "    net.plot_network()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "```\n",
    "\n",
    "The first step is to initialize the network using the constructor present in networks/information_network.py\n",
    "\n",
    "A sub-step in the initialization process is to load the data which is handled in networks/utils.py. Here, the CIFAR10 dataset is downloaded and loaded.\n",
    "\n",
    "```\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print(\"Shapes:\")\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "data_sets.data = np.concatenate((x_train, x_test), axis=0).reshape((-1, 32*32*3))\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "train_labels = np.eye(10, dtype='float32')[y_train]\n",
    "test_labels = np.eye(10, dtype='float32')[y_test]\n",
    "```\n",
    "\n",
    "Once the network is initialized, the hyperparameters of the network are printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Running/Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to train the network. This is done by calling the run_network() function in main.py. This function is present in networks/information_networks.py\n",
    "\n",
    "This function calls the train_and_calc_inf_network function which first begins the training process over the number of epochs using the batch size. \n",
    "\n",
    "\n",
    "Once the training is complete, the function proceeds to calculate the information by calling the get_information function present in information/information_process.py\n",
    "\n",
    "This function calculates the information per epoch and returns it to the train_and_calc_inf_network function which stores it in a list for future use. \n",
    "\n",
    "```\n",
    "for i in range(len(self.train_samples)):\n",
    "\tfor j in range(len(self.layers_sizes)):\n",
    "\t\tfor k in range(self.num_of_repeats):\n",
    "\t\t\tindex = i * len(self.layers_sizes) * self.num_of_repeats + j * self.num_of_repeats + k\n",
    "\t\t\tcurrent_network = results[index]\n",
    "\t\t\tself.networks[k][j][i] = current_network\n",
    "\t\t\tself.ws[k][j][i] = current_network['ws']\n",
    "\t\t\tself.weights[k][j][i] = current_network['weights']\n",
    "\t\t\tself.information[k][j][i] = current_network['information']\n",
    "\t\t\tself.grads[k][i][i] = current_network['gradients']\n",
    "\t\t\tself.test_error[k, j, i, :] = current_network['test_prediction']\n",
    "\n",
    "\n",
    "\n",
    "\t\t\tself.train_error[k, j, i, :] = current_network['train_prediction']\n",
    "\t\t\tself.loss_test[k, j, i, :] = current_network['loss_test']\n",
    "\t\t\tself.loss_train[k, j, i, :] = current_network['loss_train']\n",
    "```\n",
    "\n",
    "The functions used during this step are all the default functions that were given in the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of my CNN is the following:\n",
    "\n",
    "```\n",
    "\n",
    "# First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "\twith tf.name_scope('conv1'):\n",
    "\t\twith tf.name_scope('weights'):\n",
    "\t\t\tW_conv1 = weight_variable([5, 5, 3, 32])\n",
    "\t\t\tvariable_summaries(W_conv1)\n",
    "\t\twith tf.name_scope('biases'):\n",
    "\t\t\tb_conv1 = bias_variable([32])\n",
    "\t\t\tvariable_summaries(b_conv1)\n",
    "\t\twith tf.name_scope('activation'):\n",
    "\t\t\tinput_con1 = conv2d(x_image, W_conv1) + b_conv1\n",
    "\t\t\th_conv1 = tf.nn.relu(input_con1)\n",
    "\t\t\ttf.summary.histogram('activations', h_conv1)\n",
    "\t\twith tf.name_scope('max_pol'):\n",
    "\t\t\t# Pooling layer - downsamples by 2X.\n",
    "\t\t\th_pool1 = max_pool_2x2(h_conv1)\n",
    "\t\tinput.append(input_con1)\n",
    "\t\thidden.append(h_pool1)\n",
    "\twith tf.name_scope('conv2'):\n",
    "\t\t# Second convolutional layer -- maps 32 feature maps to 64.\n",
    "\t\twith tf.name_scope('weights'):\n",
    "\t\t\tW_conv2 = weight_variable([5, 5, 32, 64])\n",
    "\t\t\tvariable_summaries(W_conv2)\n",
    "\t\twith tf.name_scope('biases'):\n",
    "\t\t\tb_conv2 = bias_variable([64])\n",
    "\t\t\tvariable_summaries(b_conv2)\n",
    "\t\twith tf.name_scope('activation'):\n",
    "\t\t\tinput_con2 = conv2d(h_pool1, W_conv2) + b_conv2\n",
    "\t\t\th_conv2 = tf.nn.relu(input_con2)\n",
    "\t\t\ttf.summary.histogram('activations', h_conv2)\n",
    "\t\twith tf.name_scope('max_pol'):\n",
    "\t\t\t# Second pooling layer.\n",
    "\t\t\th_pool2 = max_pool_2x2(h_conv2)\n",
    "\t\tinput.append(input_con2)\n",
    "\t\thidden.append(h_pool2)\n",
    "\t# Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "\t# is down to 7x7x64 feature maps -- maps this to 1024 features.\n",
    "\twith tf.name_scope('FC1'):\n",
    "\t\twith tf.name_scope('weights'):\n",
    "\t\t\tW_fc1 = weight_variable([8 * 8 * 64, 1024])\n",
    "\t\t\tvariable_summaries(W_fc1)\n",
    "\t\twith tf.name_scope('biases'):\n",
    "\t\t\tb_fc1 = bias_variable([1024])\n",
    "\t\t\tvariable_summaries(b_fc1)\n",
    "\t\th_pool2_flat = tf.reshape(h_pool2, [-1, 8 * 8 * 64])\n",
    "\t\twith tf.name_scope('activation'):\n",
    "\t\t\tinput_fc1 = tf.matmul(h_pool2_flat, W_fc1) + b_fc1\n",
    "\t\t\th_fc1 = tf.nn.relu(input_fc1)\n",
    "\t\t\ttf.summary.histogram('activations', h_fc1)\n",
    "\n",
    "\twith tf.name_scope('drouput'):\n",
    "\t\t#keep_prob = tf.placeholder(tf.float32)\n",
    "\t\tkeep_prob = 1.0\n",
    "\t\ttf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "\t\th_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\t\tinput.append(input_fc1)\n",
    "\t\thidden.append(h_fc1_drop)\n",
    "\t# Map the 1024 features to 10 classes, one for each digit\n",
    "\twith tf.name_scope('FC2'):\n",
    "\t\twith tf.name_scope('weights'):\n",
    "\t\t\tW_fc2 = weight_variable([1024, 10])\n",
    "\t\t\tvariable_summaries(W_fc2)\n",
    "\t\twith tf.name_scope('biases'):\n",
    "\t\t\tb_fc2 = bias_variable([10])\n",
    "\t\t\tvariable_summaries(b_fc2)\n",
    "\n",
    "\tinput_y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\ty_conv = tf.nn.softmax(input_y_conv)\n",
    "\tinput.append(input_y_conv)\n",
    "\thidden.append(y_conv)\n",
    "\treturn y_conv, keep_prob, hidden, input\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Saving the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network is trained and the necessary values are calculated and stored, main.py calls the save_data() function which is present in networks/information_networks.py\n",
    "\n",
    "```\n",
    "def save_data(self, parent_dir='jobs/', file_to_save='data.pickle'):\n",
    "    \"\"\"Save the data to the file \"\"\"\n",
    "    directory = '{0}/{1}{2}/'.format(os.getcwd(), parent_dir, self.params['directory'])\n",
    "\n",
    "    data = {'information': self.information,\n",
    "            'test_error': self.test_error, 'train_error': self.train_error, 'var_grad_val': self.grads,\n",
    "            'loss_test': self.loss_test, 'loss_train': self.loss_train, 'params': self.params\n",
    "        , 'l1_norms': self.l1_norms, 'weights': self.weights, 'ws': self.ws}\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    self.dir_saved = directory\n",
    "    with open(self.dir_saved + file_to_save, 'wb') as f:\n",
    "        cPickle.dump(data, f, protocol=4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gathers all the data and saves it as a pickle file in our jobs/ directory. \n",
    "\n",
    "One change I made here was change the pickle protocol from 2 to 4 to store larger pickle files as the data generated from training the CNN on CIFAR10 is enormous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is saved, we are ready to move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Plotting the Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to plot the information plane figure. This is done by calling the plot_network() function in networks/information_network.py\n",
    "\n",
    "This leads us to the plot_figures() function present in plots/plot_figures.py.\n",
    "\n",
    "plot_figures() uses the get_data() function to retreive data from the saved pickle file.\n",
    "\n",
    "```\n",
    "def get_data(name):\n",
    "\t\"\"\"Load data from the given name\"\"\"\n",
    "\tgen_data = {}\n",
    "\t# new version\n",
    "\tif os.path.isfile(name + 'data.pickle'):\n",
    "\t\tcurent_f = open(name + 'data.pickle', 'rb')\n",
    "\t\td2 = cPickle.load(curent_f)\n",
    "\treturn d2\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is retreived, we plot the data using plot_all_epochs() function present in plots/plot_figures.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information plane graph generated by my network is attached below\n",
    "\n",
    "![](./CIFAR10-infoPlane.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some challenges I faced during this project include the following:\\\n",
    "\n",
    "- My system has a RAM of 8GB and thats why I wasn't able to run the network for more than 7 epochs without the system crashing.\n",
    "\n",
    "- The information that was calculated was stored in memory. This led to the depletion of memory as the pickle file that was generated was very substanital in size. The plot function then tries to load the pickle.data file back onto memory which was unable to be fixed.\n",
    "\n",
    "- I also attempted to run it in Google Colab. Google Colab provided me with 85 GB of RAM and GPU RAM but even that was unable to process the network for more than 7 epochs without crashing.\n",
    "\n",
    "- The codebase was extremely bug ridden and poorly documented. The modification of the codebase to work for the a newer version of Tensorflow without proper documentaion was tedious. Also, working with CIFAR10 produced a whole array of new errors that needed to be manually fixed. \n",
    "\n",
    "- Due to all of these constraints, I unfortunately was unable to figure out how to plot the gradients for the network. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
